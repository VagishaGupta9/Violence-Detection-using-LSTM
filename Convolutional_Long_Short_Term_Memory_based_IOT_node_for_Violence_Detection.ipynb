{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSji1o_OjcCW"
      },
      "source": [
        "A bit levelled up for the video classification problems?!\n",
        "\n",
        "Don't worry, we got you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5EyppKzjsJO"
      },
      "source": [
        "Insatlling library for plotting graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o0sh8Uc99dW",
        "outputId": "b8544335-535b-422a-821b-baaf911d62e2"
      },
      "source": [
        "!pip install plot-metric"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plot-metric\n",
            "  Downloading plot_metric-0.0.6-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from plot-metric) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plot-metric) (1.7.3)\n",
            "Requirement already satisfied: colorlover>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from plot-metric) (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from plot-metric) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.7/dist-packages (from plot-metric) (1.0.2)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from plot-metric) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plot-metric) (0.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->plot-metric) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->plot-metric) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->plot-metric) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->plot-metric) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.2->plot-metric) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->plot-metric) (2022.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.2->plot-metric) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->plot-metric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->plot-metric) (3.1.0)\n",
            "Installing collected packages: plot-metric\n",
            "Successfully installed plot-metric-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0POxdh41lfL-"
      },
      "source": [
        "Importing required libraries (majorly used keras and tensorflow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUUeCE2g-AaF"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDjYhQLelm2d"
      },
      "source": [
        "Of course, picking the modules we gotta play with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hag2vf-j-C3T"
      },
      "source": [
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Dense, Flatten, ConvLSTM2D,Conv2D,MaxPool2D,GlobalMaxPool2D,GlobalAveragePooling2D,BatchNormalization,Input,TimeDistributed,LSTM,Reshape,GlobalAveragePooling2D\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,roc_curve,confusion_matrix\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from plot_metric.functions import BinaryClassification\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "from tabulate import tabulate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXgT8cu-l1S3"
      },
      "source": [
        "Easy part gone. Lets get into what is required in problem.\n",
        "\n",
        "Okay, has there been any algorithm or model made for video classification???? No, I guess. But I think we heard of something known as CNN which can be applied on images.\n",
        "\n",
        "Wait, videos are composed of multiple images or frames, so it can be done by convertig videos into frames keeping in mind to maintain sequence of the frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq-Of4mOoTVw"
      },
      "source": [
        "data_directory = \"/content/drive/MyDrive/Violence Detection Dataset/Merged\"\n",
        "img_height,img_width = 112,112\n",
        "seq_len = 10\n",
        "classes = [\"NonViolence\", \"Violence\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6dpcftModjX"
      },
      "source": [
        "fps=0\n",
        "frame_count=0\n",
        "duration=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Pq98cxqwOA"
      },
      "source": [
        "Extracting 10 frames from each video to create a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snMaqNucoguS"
      },
      "source": [
        "def frames_extraction(video_path):\n",
        "    frames_list = []\n",
        "\n",
        "    vidObj = cv2.VideoCapture(video_path)\n",
        "    global fps\n",
        "    fps = vidObj.get(cv2.CAP_PROP_FPS)\n",
        "    global frame_count\n",
        "    frame_count+= int(vidObj.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    global duration\n",
        "    duration += frame_count/fps\n",
        "    count = 1\n",
        "    total_frames = vidObj.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    frames_step = total_frames//seq_len\n",
        "    for i in range(seq_len):\n",
        "        vidObj.set(1,i*frames_step)\n",
        "        success,image = vidObj.read()\n",
        "        if success:\n",
        "            image = cv2.resize(image, (img_height, img_width))\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            frames_list.append(image)\n",
        "            count += 1\n",
        "        else:\n",
        "            print(\"Defected frame\")\n",
        "            break\n",
        "\n",
        "    return frames_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbnkutgpohmz",
        "outputId": "b0ecb427-1de7-4676-de98-08638a2dc7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "def create_data(input_dir):\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    classes_list = os.listdir(input_dir)\n",
        "    for c in classes_list:\n",
        "        print(c)\n",
        "        files_list = os.listdir(os.path.join(input_dir, c))\n",
        "        for f in tqdm(files_list):\n",
        "            frames = frames_extraction(os.path.join(os.path.join(input_dir, c), f))\n",
        "            if len(frames) == seq_len:\n",
        "                X.append(frames)\n",
        "                y = [0]*len(classes)\n",
        "                y[classes.index(c)] = 1\n",
        "                Y.append(y)\n",
        "    print(\"Frames per second:\",fps)\n",
        "    print(\"Average number of frames in a video: \",frame_count/4000)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    return X, Y\n",
        "\n",
        "X, Y = create_data(data_directory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-84e09aaee9cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-84e09aaee9cc>\u001b[0m in \u001b[0;36mcreate_data\u001b[0;34m(input_dir)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mclasses_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Violence Detection Dataset/Merged'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LnJ4tCxrHkz"
      },
      "source": [
        "Saving the created data so that you need not to create it again and again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x82U5v-foluz"
      },
      "source": [
        "np.save('/content/drive/MyDrive/Violence Detection Dataset/X_112_112_10_cnn',X)\n",
        "np.save('/content/drive/MyDrive/Violence Detection Dataset/Y_112_112_10_cnn',Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLJ_X82mnfHu"
      },
      "source": [
        "Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_nhboVW-FhX"
      },
      "source": [
        "X=np.load('/content/drive/MyDrive/Violence Detection Dataset/X_112_112_10_cnn.npy')\n",
        "Y=np.load('/content/drive/MyDrive/Violence Detection Dataset/Y_112_112_10_cnn.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZIja4hPnh1F"
      },
      "source": [
        "Shuffling data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i9SqGkt-HK4"
      },
      "source": [
        "X,y = shuffle(X,Y, random_state=42)\n",
        "print(\"The shape of input is : \",np.shape(X))\n",
        "print(\"The shape of target is : \", np.shape(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhwVCjBWrK30"
      },
      "source": [
        "We gonna do 5 folds for the model and then we move forward with model.\n",
        "\n",
        "Did you miss anything?\n",
        "\n",
        "Preprocessing?\n",
        "\n",
        "No, we did it very right now go back to the block where we were trying to generate the frames. Thats the point where we preprocessed frames from BGR to RGB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdVJgih1-KI1"
      },
      "source": [
        "SIZE = (112,112)\n",
        "CHANNELS = 3\n",
        "NBFRAME = 10\n",
        "INSHAPE=(NBFRAME,) + SIZE+ (CHANNELS,)\n",
        "\n",
        "splits=5\n",
        "skf = StratifiedKFold(n_splits=splits, shuffle=False)\n",
        "y=np.argmax(y,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDJjz91orzmi"
      },
      "source": [
        "Now, a moderate difficult state, the state where we try to create model and doing layers and stuff.\n",
        "\n",
        "Since we are dumping our final model on raspberry pi, be careful with the number of parameters( not try to exceed 1.2-1.3 million.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbYArIDbskKb"
      },
      "source": [
        "So below is the CNN model, so how are we going to do it sequentially( frames to be passes in sequence) ?\n",
        "\n",
        "Lets see next block, CNN is bit easy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBWT-Ya0-2aE"
      },
      "source": [
        "def build_convnet(shape=(112, 112, 3)):\n",
        "    momentum = .9\n",
        "    model = Sequential([preprocessing.Rescaling(1.0 / 255),\n",
        "                              preprocessing.RandomFlip(\"horizontal\"),\n",
        "                              preprocessing.RandomFlip(\"vertical\"),\n",
        "                              preprocessing.RandomRotation(0.1),\n",
        "                              preprocessing.RandomZoom(0.1)])\n",
        "\n",
        "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "\n",
        "    model.add(MaxPool2D())\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "\n",
        "    model.add(MaxPool2D())\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "\n",
        "    model.add(MaxPool2D())\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU0DKbTEs89T"
      },
      "source": [
        "Here is the difficult part where we do sequence passing of frames using time distributed layer. And our modelling part is complete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL-QH5_K-_L6"
      },
      "source": [
        "def convnet_lstm_model(shape=(NBFRAME,112,112,3), nbout=2):\n",
        "    convnet = build_convnet(shape[1:])\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(nbout, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg7Nq8GKtdBq"
      },
      "source": [
        "Checking out number of parameters to see if it will fit in raspberry pi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOwUOS9s_0kY"
      },
      "source": [
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = False)\n",
        "\n",
        "model_convnet = convnet_lstm_model(INSHAPE)\n",
        "model_convnet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvZF11aqto2R"
      },
      "source": [
        "Visualizing our model in a flowchart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt4MWYRE_4do"
      },
      "source": [
        "plot_model(model_convnet,show_shapes=True,expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3DywbYxt6SK"
      },
      "source": [
        "Training the model using Adam optimizer on our data to verify training loss and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNM8Crxe9r8O"
      },
      "source": [
        "training_loss=[]\n",
        "training_accu=[]\n",
        "valid_accu=[]\n",
        "valid_loss=[]\n",
        "pred_targets=[]\n",
        "Y_test_pf=[]\n",
        "fold=1\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "  y_train=np.asarray(pd.get_dummies(y_train))\n",
        "  y_test=np.asarray(pd.get_dummies(y_test))\n",
        "\n",
        "  model_convnet = convnet_lstm_model(INSHAPE)\n",
        "  optimizer = keras.optimizers.Adam(1e-4)\n",
        "  model_convnet.compile(optimizer,'categorical_crossentropy',metrics=['acc'])\n",
        "  print(\"------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print(\"------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print(\"Fold {}\".format(fold))\n",
        "  history = model_convnet.fit(x=X_train, y=y_train, epochs=50, batch_size = 8,validation_split=0.125,callbacks = [early_stopping_callback])\n",
        "  print()\n",
        "  model_evaluation_history = model_convnet.evaluate(X_test,y_test)\n",
        "  pred_targets.append(model_convnet.predict(X_test))\n",
        "\n",
        "  training_accu.append(history.history['acc'])\n",
        "  training_loss.append(history.history['loss'])\n",
        "  valid_accu.append(history.history['val_acc'])\n",
        "  valid_loss.append(history.history['val_loss'])\n",
        "  Y_test_pf.append(y_test)\n",
        "  fold +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea6pEhn2v3o-"
      },
      "source": [
        "Looks pretty good training been done on dataset. Let's check results!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF2of6j1At2F"
      },
      "source": [
        "#**Convnet + LSTM :**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w8-1yfpAw-x"
      },
      "source": [
        "\n",
        "##**Results:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqAPfCxcwEuE"
      },
      "source": [
        "Visualizing loss and accuracy graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckPxmTHxA3YQ"
      },
      "source": [
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(history.history['loss'],'g')\n",
        "plt.plot(history.history['val_loss'],'r')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend([\"Training\",\"Validation\"])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(history.history['acc'],'g')\n",
        "plt.plot(history.history['val_acc'],'r')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.legend([\"Training\",\"Validation\"],loc=\"lower right\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EpBIou3w9fh"
      },
      "source": [
        "Various metrics such as accuracy, recall, precision, f1-score, roc and etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zZI3Lr6A9T7"
      },
      "source": [
        "avg_accuracy=0\n",
        "avg_recall=0\n",
        "avg_precision=0\n",
        "avg_f1=0\n",
        "avg_fpr=0\n",
        "avg_fnr=0\n",
        "table=[]\n",
        "for i in range(splits):\n",
        "  print(\"------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print(\"For Fold {}\".format(i+1))\n",
        "\n",
        "  y_pred=pred_targets[i]\n",
        "  y_prob=y_pred\n",
        "  y_pred=np.argmax(y_pred,axis=1)\n",
        "  Y_test=np.argmax(Y_test_pf[i],axis=1)\n",
        "  conf_mat=confusion_matrix(Y_test,y_pred)\n",
        "  table.append([\"Fold {}\".format(i+1),accuracy_score(Y_test,y_pred),f1_score(Y_test,y_pred),recall_score(Y_test,y_pred),precision_score(Y_test,y_pred),conf_mat[0][1]/(conf_mat[0][1]+conf_mat[0][0]),conf_mat[1][0]/(conf_mat[1][0]+conf_mat[1][1])])\n",
        "  avg_accuracy+=accuracy_score(Y_test,y_pred)\n",
        "  avg_recall +=recall_score(Y_test,y_pred)\n",
        "  avg_precision +=precision_score(Y_test,y_pred)\n",
        "  avg_f1 += f1_score(Y_test,y_pred)\n",
        "  avg_fpr += conf_mat[0][1]/(conf_mat[0][1]+conf_mat[0][0])\n",
        "  avg_fnr += conf_mat[1][0]/(conf_mat[1][0]+conf_mat[1][1])\n",
        "\n",
        "  print(\"Correctly predicted out of 800: \",sum(Y_test==y_pred))\n",
        "  print(\"Accuracy is                   : \",accuracy_score(Y_test,y_pred))\n",
        "  print(\"F1-score is                   : \",f1_score(Y_test,y_pred))\n",
        "  print(\"Recall score is               : \", recall_score(Y_test,y_pred))\n",
        "  print(\"Precision score is            : \", precision_score(Y_test,y_pred))\n",
        "  print(\"False Positive Rate           : \", conf_mat[0][1]/(conf_mat[0][1]+conf_mat[0][0]))\n",
        "  print(\"False Negative Rate           : \", conf_mat[1][0]/(conf_mat[1][0]+conf_mat[1][1]))\n",
        "  print(\"Area under the curve of ROC is: \",roc_auc_score(Y_test,y_prob[:,1]))\n",
        "  print()\n",
        "  print(\"Report\")\n",
        "  print(classification_report(Y_test,y_pred))\n",
        "  bc = BinaryClassification(Y_test, y_prob[:,1], labels=[\"Class 1\", \"Class 2\"])\n",
        "  plt.figure(figsize=(10,10))\n",
        "  bc.plot_roc_curve()\n",
        "  plt.show()\n",
        "  fig, ax = plot_confusion_matrix(conf_mat,show_absolute=True,show_normed=True,colorbar=True)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQDIlpujxdhg"
      },
      "source": [
        "Average ROC curve for  5 folds for CNN+LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9dwsrlENHVf"
      },
      "source": [
        "from sklearn.metrics import auc\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 101)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "for i in range(splits):\n",
        "    y_prob=pred_targets[i]\n",
        "    Y_test=np.argmax(Y_test_pf[i],axis=1)\n",
        "    aucs.append(roc_auc_score(Y_test,y_prob[:,1]))\n",
        "    fpr,tpr,_ = roc_curve(Y_test, y_prob[:,1])\n",
        "    plt.plot(fpr, tpr, alpha=0.25, label=r'Fold %d (AUC = %0.2f)' % (i+1, aucs[-1]))\n",
        "    interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "\n",
        "\n",
        "tprs = np.array(tprs)\n",
        "mean_tprs = tprs.mean(axis=0)\n",
        "std = tprs.std(axis=0)\n",
        "\n",
        "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
        "tprs_lower = np.maximum(mean_tprs - std,0)\n",
        "\n",
        "mean_auc=auc(mean_fpr, mean_tprs)\n",
        "std_auc=np.std(aucs)\n",
        "plt.plot(mean_fpr, mean_tprs, 'b',lw=2,label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc))\n",
        "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.4,label=r'$\\pm$ 1 std. dev.')\n",
        "plt.title(\"Average ROC for CNN+LSTM\")\n",
        "plt.plot([0, 1], [0, 1],'r--',label='Random Guess',lw=2)\n",
        "plt.xlim([-0.01, 1.01])\n",
        "plt.ylim([-0.01, 1.01])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCAzQtBUyIcA"
      },
      "source": [
        "Making table for various folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygGEMKRVlkKj"
      },
      "source": [
        "tbl=np.array(table)\n",
        "table.append(['Average',(avg_accuracy/splits),(avg_f1/splits),(avg_recall/splits),(avg_precision/splits),(avg_fpr/splits),(avg_fnr/splits)])\n",
        "table.append(['Standard Deviation',np.std(tbl[:,1].astype('float32')),np.std(tbl[:,2].astype('float32')),np.std(tbl[:,3].astype('float32')),np.std(tbl[:,4].astype('float32')),np.std(tbl[:,5].astype('float32')),np.std(tbl[:,6].astype('float32'))])\n",
        "print(tabulate(table,headers=[\"\",\"Accuracy\",\"F1-score\",\"Recall\",\"Precison\",\"FPR\",\"FNR\"],tablefmt='github'))\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}